{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(artifact_path):\n",
    "    results_list = []\n",
    "    for file_name in os.listdir(artifact_path):\n",
    "        if file_name.startswith(\"metrics\") and file_name.endswith(\".yml\"):\n",
    "            # Load the YAML file into a Python dictionary\n",
    "            file_path = os.path.join(artifact_path, file_name)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                results_list.append(yaml.safe_load(f))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_lr(artifact_path):\n",
    "    loss_path = os.path.join(os.path.dirname(artifact_path), \"metrics\", \"loss\")\n",
    "    lr_path = os.path.join(os.path.dirname(artifact_path), \"metrics\", \"lr\")\n",
    "\n",
    "    timestamps = None\n",
    "    loss = None\n",
    "    lr = None\n",
    "    epoch_number = None\n",
    "\n",
    "    if os.path.exists(loss_path):\n",
    "        with open(loss_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            # read in each line as \"timestamps\", \"loss\", and \"epoch number\"\n",
    "            timestamps = [float(line.split(\" \")[0]) for line in lines]\n",
    "            loss = [float(line.split(\" \")[1]) for line in lines]\n",
    "            epoch_number = [int(line.split(\" \")[2]) for line in lines]\n",
    "\n",
    "    if os.path.exists(lr_path):\n",
    "        with open(lr_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            lr = [float(line.split(\" \")[1]) for line in lines]\n",
    "\n",
    "    return timestamps, loss, lr, epoch_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(nested_dict, path=[]):\n",
    "    items = []\n",
    "    for key, value in nested_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            items.extend(flatten_dict(value, path + [key]))\n",
    "        elif isinstance(value, list):\n",
    "            for subvalue in value:\n",
    "                items.extend(flatten_dict(subvalue, path + [key]))\n",
    "        else:\n",
    "            items.append(path + [key, value])\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_organized_results(results_dict):\n",
    "    results_all = flatten_dict(results_dict)\n",
    "    results_organized = dict()\n",
    "    for result in results_all:\n",
    "        if result[1] not in results_organized:\n",
    "            results_organized[result[1]] = dict()\n",
    "        if result[2] not in results_organized[result[1]]:\n",
    "            results_organized[result[1]][result[2]] = []\n",
    "        results_organized[result[1]][result[2]].append([result[0].split(\"_\"), eval(result[3])])\n",
    "    return results_organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results(results_organized, model_name, encoding_type):\n",
    "    for key, val in results_organized.items():\n",
    "        for k, v in val.items():\n",
    "            results = [[int(r[0][3]), r[1]] for r in v if r[0][1] in model_name and r[0][2] in encoding_type]\n",
    "            runids = [r[0][0] for r in v if r[0][1] in model_name and r[0][2] in encoding_type]\n",
    "            results_organized[key][k] = [results, runids]\n",
    "    return results_organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dict_subset_dict(check_dict,filter_dict):\n",
    "    include_flag = True\n",
    "    for filter_cat, filter_val in filter_dict.items():\n",
    "        assert filter_cat in check_dict, f\"{filter_cat} not in {check_dict}\"\n",
    "        for cat_key, cat_val in filter_val.items():\n",
    "            if isinstance(check_dict[filter_cat],str):\n",
    "                check_dict_from_str = eval(check_dict[filter_cat])\n",
    "            else: \n",
    "                check_dict_from_str = check_dict[filter_cat]\n",
    "            assert cat_key in check_dict_from_str, f\"{cat_key} not in {check_dict[filter_cat]}\"\n",
    "            # print(eval(check_dict[filter_cat])[cat_key],cat_val)\n",
    "            if check_dict_from_str[cat_key] != cat_val:\n",
    "                    include_flag = False\n",
    "                    break\n",
    "            if not include_flag:\n",
    "                break\n",
    "    return include_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trained_dict(trained_dict,filter_dict,filtered_trained_dict=None):\n",
    "    if filtered_trained_dict is None:\n",
    "        filtered_trained_dict = dict()\n",
    "    for runid, val in trained_dict.items():\n",
    "        run_params = val[1]\n",
    "        if is_dict_subset_dict(run_params,filter_dict):\n",
    "            # print(f\"including {runid}\")\n",
    "            filtered_trained_dict[runid] = val\n",
    "    return filtered_trained_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_dict():\n",
    "\n",
    "    # Connect to the MLflow tracking server\n",
    "    mlflow.set_tracking_uri(\"\")\n",
    "\n",
    "    trained_dict = dict()\n",
    "    loss_lr = dict()\n",
    "    # Iterate over all experiments\n",
    "    for exp in mlflow.search_experiments():\n",
    "        # Get the experiment name and ID\n",
    "        exp_name = exp.name\n",
    "        exp_id = exp.experiment_id\n",
    "        # if exp_id == \"42\":\n",
    "        #     continue\n",
    "        # print(f\"Experiment '{exp_name}' (ID: {exp_id})\")\n",
    "\n",
    "        # Load the experiment using the ID\n",
    "        exp = mlflow.get_experiment(exp_id)\n",
    "\n",
    "        # Iterate over all runs in the experiment\n",
    "        runs_df = mlflow.search_runs(exp_id)\n",
    "        for i, runid in enumerate(runs_df.run_id):\n",
    "            # Get the run ID, metrics, and parameters\n",
    "\n",
    "            run = mlflow.get_run(runid)\n",
    "            n_epochs = eval(run.data.params[\"loader\"])[\"n_epochs\"]\n",
    "            results_key = f\"{runid}\"\n",
    "\n",
    "            artifact_path = run.info.artifact_uri\n",
    "            if artifact_path.startswith(\"file://\"):\n",
    "                artifact_path = artifact_path.replace(\"file://\", \"\")\n",
    "            timestamps, loss, lr, epoch_number = get_loss_lr(artifact_path)\n",
    "\n",
    "            if (\n",
    "                loss is not None\n",
    "                and len(loss) == n_epochs\n",
    "                and run.info.status == \"FINISHED\"\n",
    "            ):\n",
    "                loss_lr[results_key] = {\n",
    "                    \"loss\": loss,\n",
    "                    \"lr\": lr,\n",
    "                    \"epoch_number\": epoch_number,\n",
    "                    \"timestamps\": timestamps,\n",
    "                }\n",
    "                trained_dict[results_key] = [artifact_path, run.data.params]\n",
    "                # print(\n",
    "                #     f\"{runid}, status: {run.info.status}\"\n",
    "                # )\n",
    "    return trained_dict, loss_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_dict(filtered_trained_dict,evaluate_filter,results_dict = None):\n",
    "    if results_dict == None:\n",
    "        results_dict = dict()\n",
    "    for runid , val in filtered_trained_dict.items():\n",
    "        artifact_path = val[0]\n",
    "        run_params = val[1]\n",
    "\n",
    "        # Get the run ID, metrics, and parameters\n",
    "        \n",
    "        for file_name in os.listdir(artifact_path):\n",
    "            if file_name.startswith(\"eval\") and file_name.endswith(\".yml\"):\n",
    "                # Load the YAML file into a Python dictionary\n",
    "                file_path = os.path.join(artifact_path, file_name)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    check_dict = yaml.safe_load(f)\n",
    "                    # print(check_dict)\n",
    "                    # print(evaluate_filter)\n",
    "                    if is_dict_subset_dict(check_dict,evaluate_filter):\n",
    "                        \n",
    "                        model_name = eval(run_params[\"model\"])[\"name\"]\n",
    "                        encoding_type = eval(run_params[\"model\"])[\"encoding\"]\n",
    "                        num_bins = eval(run_params[\"model\"])[\"num_bins\"]\n",
    "                        n_epochs = eval(run_params[\"loader\"])[\"n_epochs\"]\n",
    "                        results_key = f\"{runid}_{model_name}_{encoding_type}_{num_bins}\"\n",
    "                        \n",
    "                        metric_file_name = file_name.replace(\"eval\",\"metrics\")\n",
    "                        metric_file_path = os.path.join(artifact_path, metric_file_name)\n",
    "                        if os.path.exists(metric_file_path):\n",
    "                            with open(metric_file_path, \"r\") as f:    \n",
    "                                results_dict[results_key] = [yaml.safe_load(f)]\n",
    "                                # print(runid, file_name)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results,cgen):\n",
    "\n",
    "    for metric in results:\n",
    "        \n",
    "        if metric != \"AEE\":\n",
    "            continue\n",
    "        for i, dataset in enumerate(results[metric]):\n",
    "            x = np.array(\n",
    "                [results[metric][dataset][0][i] for i, rid in enumerate(results[metric][dataset][1]) if rid == \"EVFlowNet\"]\n",
    "            )\n",
    "            if(x.shape[0] != 0):\n",
    "                \n",
    "                x = x[x[:, 0].argsort()]\n",
    "                plt.plot(x[:, 0], x[:, 1], \"s\", color=cgen[i])\n",
    "            x = np.array(\n",
    "                [results[metric][dataset][0][i] for i, rid in enumerate(results[metric][dataset][1]) if rid != \"EVFlowNet\"]\n",
    "            )\n",
    "            if(x.shape[0] != 0):\n",
    "                x = x[x[:, 0].argsort()]\n",
    "                plt.plot(x[:, 0], x[:, 1], \"o-\", color=cgen[i], label=dataset)\n",
    "\n",
    "        plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "        plt.xlabel(\"number of (bins) frames\")\n",
    "        plt.ylabel(\"Error [pixels]\")\n",
    "        plt.title(f\"{metric} for {model_name} with {encoding_type} encoding\")\n",
    "        plt.grid(which=\"major\", color=\"#666666\", linestyle=\"-\")\n",
    "        plt.minorticks_on()\n",
    "        plt.grid(which=\"minor\", color=\"#999999\", linestyle=\"-\", alpha=0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter_dict = {\n",
    "    'model':\n",
    "        {\n",
    "            'name': 'RecEVFlowNet', \n",
    "            # 'encoding': 'voxel', \n",
    "            # 'round_encoding': False, \n",
    "            # 'norm_input': False, \n",
    "            # 'num_bins': 15, \n",
    "            # 'base_num_channels': 32, \n",
    "            # 'kernel_size': 5, \n",
    "            # 'activations': ['relu', None], \n",
    "            # 'mask_output': True\n",
    "        },\n",
    "    'optimizer':\n",
    "        {\n",
    "            'name': 'Adam', \n",
    "            'lr': 0.00002\n",
    "        },\n",
    "    'data':\n",
    "        {\n",
    "            'mode': 'events', \n",
    "            'window': 1000, \n",
    "            'window_loss': 10000\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_filter_dict = {\n",
    "    'data':\n",
    "        {   \n",
    "            'mode': 'gtflow_dt1',\n",
    "            'path': 'datasets/data/MVSEC/',\n",
    "            # 'window': 1,\n",
    "            'window_eval': 15000,\n",
    "            'window_loss': 10000\n",
    "        },\n",
    "    'loader':\n",
    "        {\n",
    "            'augment': [],\n",
    "            'batch_size': 1,\n",
    "            'gpu': 0,\n",
    "            'n_epochs': 100,\n",
    "            'resolution': [256,256],\n",
    "            'seed': 0\n",
    "        },\n",
    "    'loss':\n",
    "        {\n",
    "            'clip_grad': 100.0,\n",
    "            'flow_regul_weight': 0.001,\n",
    "            'overwrite_intermediate': False\n",
    "        },\n",
    "    'metrics':\n",
    "        {\n",
    "            'flow_scaling': 128,\n",
    "            'name': ['AEE'],\n",
    "        },\n",
    "    'model':\n",
    "        {\n",
    "            'activations': ['relu', None],       \n",
    "            'base_num_channels': 32,\n",
    "            'encoding': 'voxel',\n",
    "            'kernel_size': 3,\n",
    "            'mask_output': True,\n",
    "            'name': 'RecEVFlowNet',\n",
    "            'norm_input': False,\n",
    "    #         'num_bins': 15,\n",
    "            'round_encoding': False,\n",
    "            'spiking_neuron': 'None'\n",
    "        },\n",
    "    'optimizer':\n",
    "        {\n",
    "             'lr': 0.0002,\n",
    "            'name': 'Adam'\n",
    "        },\n",
    "    'vis':\n",
    "        {\n",
    "            'activity': False,\n",
    "            'bars': True,\n",
    "            'enabled': False,\n",
    "            'px': 400,\n",
    "            'store': False,\n",
    "            'store_grads': False,\n",
    "            'verbose': False\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "NUM_COLORS = 5\n",
    "\n",
    "cm = pylab.get_cmap(\"gist_rainbow\")\n",
    "for i in range(NUM_COLORS):\n",
    "    color = cm(1.0 * i / NUM_COLORS)  # color will now be an RGBA tuple\n",
    "\n",
    "# or if you really want a generator:\n",
    "cgen = [cm(1.0 * i / NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "\n",
    "cgen = ['#1f77b4',\n",
    "'#ff7f0e',\n",
    "'#2ca02c',\n",
    "'#d62728',\n",
    "'#9467bd',\n",
    "'#8c564b']\n",
    "\n",
    "model_name = [\"RecEVFlowNet\"]\n",
    "encoding_type = [\"cnt\", \"voxel\"]\n",
    "\n",
    "train_filter_dict = {\n",
    "    'model':\n",
    "        {\n",
    "            # 'name': 'RecEVFlowNet', \n",
    "            # 'encoding': 'voxel',\n",
    "        },\n",
    "    'optimizer':\n",
    "        {\n",
    "            'name': 'Adam', \n",
    "            # 'lr': 0.00002\n",
    "        },\n",
    "    'data':\n",
    "        {\n",
    "            'mode': 'events', \n",
    "            'window': 1000, \n",
    "            'window_loss': 10000\n",
    "        }\n",
    "    }\n",
    "trained_dict,_ = get_trained_dict()\n",
    "filtered_trained_dict = filter_trained_dict(trained_dict,train_filter_dict)\n",
    "# if you want to add other runs, you can do so by\n",
    "# filtered_trained_dict = filter_trained_dict(trained_dict,train_filter_dict,filtered_trained_dict)\n",
    "# for runid in filtered_trained_dict:\n",
    "#     print(runid)\n",
    "print('For every 1 GT frame:')\n",
    "evaluate_filter_dict = {'data':{'mode': 'gtflow_dt1'}}\n",
    "results_dict = get_results_dict(filtered_trained_dict,evaluate_filter_dict)\n",
    "results = filter_results(get_organized_results(results_dict), model_name, encoding_type)\n",
    "plot_results(results,cgen)\n",
    "evaluate_filter_dict = {'data':{'mode': 'gtflow_dt4'}}\n",
    "print('For every 4 GT frame:')\n",
    "results_dict = get_results_dict(filtered_trained_dict,evaluate_filter_dict)\n",
    "results = filter_results(get_organized_results(results_dict), model_name, encoding_type)\n",
    "plot_results(results,cgen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 'Default' (ID: 397965458947131238)\n",
      "3fe96d3b89604adab4ba6683368c1296,  model name: RecEVFlowNet, encoding: voxel, and num_bins: 15, status: FINISHED\n",
      "bf8c0225db634b0f899ab908568e246b,  model name: RecEVFlowNet, encoding: voxel, and num_bins: 11, status: FINISHED\n",
      "edaa3898403942ee983da430eaa1de76,  model name: RecEVFlowNet, encoding: voxel, and num_bins: 9, status: FINISHED\n",
      "e2f54e90e1494d179662b74c1c289e23,  model name: RecEVFlowNet, encoding: voxel, and num_bins: 7, status: FINISHED\n",
      "5f82cda3d0e346c39e3f9f57b4dc5608,  model name: RecEVFlowNet, encoding: voxel, and num_bins: 5, status: FINISHED\n",
      "a597ab170aff4a0b82c103d02bfdb4f9,  model name: RecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "e9e83b9faa214381bc568c0b40abd8e4,  model name: RecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "Experiment 'submission' (ID: 42)\n",
      "LIFFireFlowNet,  model name: LIFFireFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "LeakyFireFlowNet,  model name: LeakyFireFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "FireFlowNet,  model name: FireFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "XLIFEVFlowNet,  model name: XLIFRecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "ALIFEVFlowNet,  model name: ALIFRecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "PLIFEVFlowNet,  model name: PLIFRecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "LIFEVFlowNet,  model name: SpikingRecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "LeakyEVFlowNet,  model name: LeakyRecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "LeakyFireNet,  model name: LeakyFireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "RNNFireNet,  model name: RNNFireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "XLIFFireNet,  model name: XLIFFireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "PLIFFireNet,  model name: PLIFFireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "RNNEVFlowNet,  model name: RNNRecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "ALIFFireNet,  model name: ALIFFireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "LIFFireNet,  model name: LIFFireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "EVFlowNet,  model name: RecEVFlowNet, encoding: cnt, and num_bins: 2, status: FINISHED\n",
      "FireNet,  model name: FireNet, encoding: cnt, and num_bins: 2, status: FINISHED\n"
     ]
    }
   ],
   "source": [
    "# Connect to the MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"\")\n",
    "\n",
    "results_dict = dict()\n",
    "results_loss_lr = dict()\n",
    "# Iterate over all experiments\n",
    "for exp in mlflow.search_experiments():\n",
    "    # Get the experiment name and ID\n",
    "    exp_name = exp.name\n",
    "    exp_id = exp.experiment_id\n",
    "    # if exp_id == \"42\":\n",
    "    #     continue\n",
    "    print(f\"Experiment '{exp_name}' (ID: {exp_id})\")\n",
    "\n",
    "    # Load the experiment using the ID\n",
    "    exp = mlflow.get_experiment(exp_id)\n",
    "\n",
    "    # Iterate over all runs in the experiment\n",
    "    runs_df = mlflow.search_runs(exp_id)\n",
    "    for i, runid in enumerate(runs_df.run_id):\n",
    "        # Get the run ID, metrics, and parameters\n",
    "\n",
    "        run = mlflow.get_run(runid)\n",
    "        model_name = eval(run.data.params[\"model\"])[\"name\"]\n",
    "        encoding_type = eval(run.data.params[\"model\"])[\"encoding\"]\n",
    "        num_bins = eval(run.data.params[\"model\"])[\"num_bins\"]\n",
    "        n_epochs = eval(run.data.params[\"loader\"])[\"n_epochs\"]\n",
    "        results_key = f\"{runid}_{model_name}_{encoding_type}_{num_bins}\"\n",
    "\n",
    "        artifact_path = run.info.artifact_uri\n",
    "        if artifact_path.startswith(\"file://\"):\n",
    "            artifact_path = artifact_path.replace(\"file://\", \"\")\n",
    "        timestamps, loss, lr, epoch_number = get_loss_lr(artifact_path)\n",
    "\n",
    "        if (\n",
    "            loss is not None\n",
    "            and len(loss) == n_epochs\n",
    "            and run.info.status == \"FINISHED\"\n",
    "            and (num_bins < 3 or lr[0] < 0.0002)\n",
    "        ):\n",
    "            results_loss_lr[results_key] = {\n",
    "                \"loss\": loss,\n",
    "                \"lr\": lr,\n",
    "                \"epoch_number\": epoch_number,\n",
    "                \"timestamps\": timestamps,\n",
    "            }\n",
    "            results_dict[results_key] = get_results(artifact_path)\n",
    "            print(\n",
    "                f\"{runid},  model name: {model_name}, encoding: {encoding_type}, and num_bins: {num_bins}, status: {run.info.status}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "converged_runid = \"a597ab170aff4a0b82c103d02bfdb4f9_RecEVFlowNet_cnt_2\"\n",
    "non_converged_runid = \"e9e83b9faa214381bc568c0b40abd8e4_RecEVFlowNet_cnt_2\"\n",
    "\n",
    "fig, (ax_loss1, ax_loss2, ax_lrs) = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "ax_loss1.plot(results_loss_lr[non_converged_runid][\"loss\"], color=\"blue\", label=\"not converged\")\n",
    "ax_loss1.set_ylabel(\"loss\")\n",
    "ax_loss1.set_xlabel(\"Epoch\")\n",
    "ax_loss1.set_title(\"Loss curves\")\n",
    "ax_loss1.legend()\n",
    "ax_loss1.grid(which=\"major\", color=\"#666666\", linestyle=\"-\")\n",
    "ax_loss1.minorticks_on()\n",
    "ax_loss1.grid(which=\"minor\", color=\"#999999\", linestyle=\"-\", alpha=0.2)\n",
    "\n",
    "\n",
    "ax_loss2.plot(results_loss_lr[converged_runid][\"loss\"], color=\"red\", label=\"converged\")\n",
    "ax_loss2.set_ylabel(\"loss\")\n",
    "ax_loss2.set_xlabel(\"Epoch\")\n",
    "ax_loss2.set_title(\"Loss curves\")\n",
    "ax_loss2.legend()\n",
    "ax_loss2.grid(which=\"major\", color=\"#666666\", linestyle=\"-\")\n",
    "ax_loss2.minorticks_on()\n",
    "ax_loss2.grid(which=\"minor\", color=\"#999999\", linestyle=\"-\", alpha=0.2)\n",
    "\n",
    "\n",
    "ax_lrs.plot(results_loss_lr[non_converged_runid][\"lr\"], color=\"blue\", label=\"not converged\")\n",
    "ax_lrs.plot(results_loss_lr[converged_runid][\"lr\"], color=\"red\", label=\"converged\")\n",
    "ax_lrs.set_ylabel(\"lr\")\n",
    "ax_lrs.set_xlabel(\"Epoch\")\n",
    "ax_lrs.set_title(\"Learning rates\")\n",
    "ax_lrs.legend()\n",
    "ax_lrs.grid(which=\"major\", color=\"#666666\", linestyle=\"-\")\n",
    "ax_lrs.minorticks_on()\n",
    "ax_lrs.grid(which=\"minor\", color=\"#999999\", linestyle=\"-\", alpha=0.2)\n",
    "\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "of-b2eb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
